<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>Fine-tune the Pretrained ATST Model for Sound Event Detection | EtherSpace of Nian</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://saoyear.github.io/favicon.ico?v=1717479538719">
<link rel="stylesheet" href="https://saoyear.github.io/styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="[Accepted by ICASSP 2024] We introduce a fine-tuning method for the pretrained model (ATST-Frame) integrated in the SED ..." />
    <meta name="keywords" content="Publications" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://saoyear.github.io">
        <img src="https://saoyear.github.io/images/avatar.png?v=1717479538719" class="site-logo">
        <h1 class="site-title">EtherSpace of Nian</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            Home
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            Publications
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            Blogs
          </a>
        
      
        
          <a href="https://saoyear.github.io/post/about_me" class="site-nav">
            About me
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
          <a class="social-link" href="https://github.com/SaoYear" target="_blank">
            <i class="fab fa-github"></i>
          </a>
        
      
        
      
        
      
        
          <a class="social-link" href="https://www.zhihu.com/people/saoyear" target="_blank">
            <i class="fab fa-zhihu"></i>
          </a>
        
      
        
      
    </div>
    <div class="site-description">
      Welcome to my site!
<br />
I am a PhD. student @ Westlake University. My research interests include <b> sound event detection </b>;  <b>semi-supervised / self-supervised learning </b> in audio processing. <br /> 
Feel free to contact me: sao_year@126.com
    </div>
    <div class="site-footer">
      Nian Shao, PhD. student @ Westlake University | <a class="rss" href="https://saoyear.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">Fine-tune the Pretrained ATST Model for Sound Event Detection</h2>
            <div class="post-date">2023-09-20</div>
            
              <div class="feature-container" style="background-image: url('https://saoyear.github.io/post-images/finetune_ATST_SED.png')">
              </div>
            
            <div class="post-content" v-pre>
              <p>[Accepted by ICASSP 2024] We introduce a fine-tuning method for the pretrained model (<a href="https://arxiv.org/abs/2306.04186">ATST-Frame</a>) integrated in the SED system. And we obtain new SOTA performances on the DESED development dataset.</p>
<h3 id="authors">Authors</h3>
<p>Nian Shao; Xian Li; Xiaofei Li</p>
<!-- more -->
<h3 id="abstract">Abstract</h3>
<p>Sound event detection (SED) often suffers from the data deficiency problem. The recent baseline system in the DCASE2023 challenge task 4 leverages the large pretrained self-supervised learning (SelfSL) models to mitigate such restriction, where the pretrained models help to produce more discriminative features for SED. However , the pretrained models are regarded as a frozen feature extractor in the challenge baseline system and most of the challenge submissions , and fine-tuning of the pretrained models has been rarely studied. In this work, we study the fine-tuning method of the pre-trained models for SED. We first introduce ATST-Frame, our newly proposed SelfSL model, to the SED system. ATST-Frame was especially designed for learning frame-level representations of audio signals and obtained state-of-the-art (SOTA) performances on a series of downstream tasks. We then propose a fine-tuning method for ATST-Frame using both (in-domain) unlabelled and labelled SED data. Our experiments show that, the proposed method overcomes the overfitting problem when fine-tuning the large pretrained network , and our SED system obtains new SOTA results of 0.587/0.812 PSDS1/PSDS2 scores on the DCASE challenge task 4 dataset.</p>
<h3 id="url">URL</h3>
<p>https://arxiv.org/pdf/2309.08153v1.pdf</p>
<h3 id="code">Code</h3>
<p><a href="https://github.com/Audio-WestlakeU/ATST-SED">Published on github</a></p>
<h3 id="strategy-overflow">Strategy Overflow</h3>
<figure data-type="image" tabindex="1"><img src="https://saoyear.github.io/post-images/1695192523843.png" alt="" loading="lazy"></figure>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://saoyear.github.io/tag/LxSu5Vybm/" class="tag">
                    Publications
                  </a>
                
              </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://saoyear.github.io/post/FS_EEND/">
                  <h3 class="post-title">
                    Frame-wise Streaming End-to-end Speaker Diarization with Non-autoregressive Self-attention-based Attractors
                  </h3>
                </a>
              </div>
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>





  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: 'd110d408179ab00c332f',
        clientSecret: 'b054f3e6b2ad769c3d0a095dcf58d74f5884906b',
        repo: 'SaoYear.github.io',
        owner: 'SaoYear',
        admin: ['SaoYear'],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
